{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"System Design Interview Ready Notes \ud83d\ude80","text":"<p>System design study notes prepared from various resources including Interviewready, tech blogs, and specialized books. These notes cover fundamental concepts, architectural patterns, and practical strategies for building scalable systems.</p>"},{"location":"#index","title":"\ud83d\udcda Index","text":""},{"location":"#foundations","title":"\ud83c\udfc1 Foundations","text":"<ul> <li>System Design Foundations - Core principles and high-level overview.</li> <li>Reliability - Understanding availability, fault tolerance, and system health.</li> </ul>"},{"location":"#networking-traffic-management","title":"\ud83c\udf10 Networking &amp; Traffic Management","text":"<ul> <li>DNS (Domain Name System) - How the internet resolves addresses.</li> <li>Load Balancing - Distributing traffic across servers and algorithms.</li> <li>CDN (Content Delivery Network) - Global content delivery and latency reduction.</li> <li>Rate Limiting - Protecting services from abuse and traffic spikes.</li> </ul>"},{"location":"#performance-scalability","title":"\u26a1 Performance &amp; Scalability","text":"<ul> <li>Distributed Caching - Caching strategies, eviction policies, and consistent hashing.</li> <li>High Latency - Identifying and mitigating performance bottlenecks.</li> <li>Handling Traffic Surges - Strategies for managing sudden load increases.</li> </ul>"},{"location":"#architecture-deployment","title":"\ud83c\udfd7\ufe0f Architecture &amp; Deployment","text":"<ul> <li>Microservices - Decoupling systems into smaller, manageable services.</li> </ul>"},{"location":"#visual-guides-diagrams","title":"\ud83d\udcca Visual Guides &amp; Diagrams","text":"<p>The repository contains several diagrams to help visualize complex concepts:</p> <ul> <li>Caching: Consistent Hashing, Cache Architecture</li> <li>Deployment: Local vs Cloud, Server vs Serverless</li> <li>Traffic: Load Balancing Algorithms, Rate Limiting</li> </ul> <p>Note: These notes are prepared from resources like Interviewready. These are work in progress and will be updated regularly. Some of these notes might be refined with AI for clarity.</p>"},{"location":"CDN/","title":"CDN","text":""},{"location":"CDN/#how-do-you-add-new-files-to-cdn","title":"How do you add new files to CDN ?","text":"<p>A. In your file system(either local like Hadoop or in cloud like S3) you have a dedicated folder which is being checked by the CDN service for any new file. This check could be periodic, which happens every 10 seconds or so. Once it sees a new file, it sends that file and its metadata to their cloud computer which pushes to all the CDN servers across the globe.</p>"},{"location":"CDN/#why-not-to-use-cdn-servers-for-storing-dynamic-content","title":"Why not to use CDN Servers for storing dynamic content ?","text":"<p>A. For example let's say some user places an order for 10 units in US, some user places an order of 50 units in India. While you had only 50 units total. Then when both of them will get processed it will fail as it is going beyond the total available. So for dynamic data it is best to have data in database as a single source of truth.</p> <p>Examples of CDN: Akamai, AWS Cloudfront, Cloudflare</p>"},{"location":"Caching/","title":"Distributed Caching and Performance Architecture","text":""},{"location":"Caching/#core-metric","title":"Core Metric","text":"<p>Cache performance is measured by Average Memory Access Time (AMAT):</p> \\[AMAT = Hit \\ Time + (Miss \\ Rate \\times Miss \\ Penalty)\\] <p>To optimize AMAT, one must tune four key factors:</p> <ul> <li>Eviction Policy</li> <li>Write Policy</li> <li>Data Distribution</li> <li>Cache Placement.</li> </ul>"},{"location":"Caching/#1-eviction-policies-replacement-policies","title":"1. Eviction Policies (Replacement Policies)","text":"<p>When the cache reaches capacity (is full), these policies dictate which specific data items must be removed (evicted) to make space for new incoming data,.</p>"},{"location":"Caching/#a-lru-least-recently-used","title":"A. LRU (Least Recently Used)","text":"<ul> <li>Definition: This policy identifies and evicts the item that has not been accessed for the longest duration of time. It relies heavily on the principle of Temporal Locality,.</li> <li>Standard Example: A web browser history. If you visit Site A, then B, then C, then D, and the cache is full, Site A (the earliest visited) is evicted first.</li> <li>Specific Use Case: Social Media Feeds (e.g., Twitter/Instagram).</li> <li>Reasoning:</li> <li>Social media users prioritize recency.</li> <li>A post may have high historical engagement (e.g., 1 million likes) from 2 days ago, but it is less relevant to the user than a post from 1 hour ago.</li> <li>LRU ensures content remains fresh in the feed,.</li> </ul>"},{"location":"Caching/#b-lfu-least-frequently-used","title":"B. LFU (Least Frequently Used)","text":"<ul> <li>Definition: This policy tracks the number of times a block or item is accessed. The block with the fewest total accesses (hits) is selected for eviction,.</li> <li>Standard Example: A Content Delivery Network (CDN) storing video files. A viral video is kept; a one-time obscure video is dropped.</li> <li>Specific Use Case: E-Commerce Catalogs.</li> <li>Reasoning:</li> <li>Popular products (e.g., a standard iPhone model) remain popular for long periods.</li> <li>LFU provides stability by preventing established popular items from being evicted by a momentary surge of new, less important items,.</li> <li>Downside: It is susceptible to \"Cache Pollution.\" An item that was extremely popular last month might stay in the cache forever due to a high historical frequency count, even if no one is watching or accessing it today.</li> </ul>"},{"location":"Caching/#c-fifo-first-in-first-out","title":"C. FIFO (First-In, First-Out)","text":"<ul> <li>Definition: This policy evicts the oldest block added to the cache, regardless of how recently or frequently it has been used,.</li> <li>Example: A printer buffer queue.</li> <li>Downside: This policy can suffer from Belady's Anomaly, a phenomenon where increasing the amount of available cache memory actually increases the miss rate.</li> </ul>"},{"location":"Caching/#d-random-replacement","title":"D. Random Replacement","text":"<ul> <li>Definition: The system randomly selects a victim block to evict.</li> <li>Example: ARM processors.</li> <li>Reasoning: This is used in hardware environments where the overhead of tracking LRU history is too expensive relative to the performance gain.</li> </ul>"},{"location":"Caching/#2-write-policies","title":"2. Write Policies","text":"<p>These policies determine how data consistency is managed between the Cache and the Database (DB) when data is modified.</p>"},{"location":"Caching/#a-write-through","title":"A. Write-Through","text":"<ul> <li>Mechanism: Updates are written to the Cache and the DB simultaneously,.</li> <li>Decision Criterion: Use this when Data Integrity is paramount (Safety &gt; Speed),.</li> <li>Specific Example: Financial/Banking transactions. You cannot afford to lose a deposit record,.</li> <li>Pros: Strong consistency; the Database is always up-to-date.</li> <li>Cons: High latency, as the write speed is limited by the slower Database performance,.</li> </ul>"},{"location":"Caching/#b-write-behind-write-back","title":"B. Write-Behind (Write-Back)","text":"<ul> <li>Mechanism: Updates are written only to the Cache initially. The Database is updated later (asynchronously),.</li> <li>Decision Criterion: Use this when Write Speed is paramount (Speed &gt; Safety),.</li> <li>Specific Example: Real-time analytics counters (e.g., YouTube view counts, \"700 users watching\"). Losing a few increments is acceptable in exchange for performance,.</li> <li>Pros: Extremely low latency.</li> <li>Cons: Data Loss Risk. If the cache crashes before it syncs with the DB, the updates are permanently lost,.</li> </ul>"},{"location":"Caching/#c-write-around","title":"C. Write-Around","text":"<ul> <li>Mechanism: Data writes bypass the cache entirely and go directly to the DB. Data is only loaded into the cache if it is read later,.</li> <li>Decision Criterion: Use for \"Write-Once, Read-Rarely\" data.</li> <li>Specific Example: Log files, Data Backups, or uploading large media files,.</li> <li>Pros: This prevents \"Cache Flooding\" (filling the valuable cache space with useless data that will not be read again),.</li> </ul>"},{"location":"Caching/#d-write-aside-look-aside","title":"D. Write-Aside (Look-Aside)","text":"<ul> <li>Mechanism: The application code controls the flow. It checks the cache; if the data is missing, the application reads from the DB and manually updates the cache.</li> <li>Use Case: General-purpose caching patterns (e.g., Memcached).</li> </ul>"},{"location":"Caching/#common-challenges-solutions","title":"Common Challenges &amp; Solutions","text":"<ul> <li>Inconsistent Data: The DB and Cache hold different values.</li> <li>Solution: Update the cache infrequently using Time-To-Live (TTL) or utilize a Write-Through policy.</li> <li>Stale Reads: An update hits the DB, but subsequent reads still hit the old data in the Cache.</li> <li>Solution: Ensure the write policy invalidates (deletes) the cache entry immediately upon a write operation.</li> </ul>"},{"location":"Caching/#3-data-distribution-content-strategy","title":"3. Data Distribution (Content Strategy)","text":"<p>Optimizing what you cache is just as critical as how you cache it.</p>"},{"location":"Caching/#a-optimization-the-heavy-object-problem","title":"A. Optimization: The \"Heavy Object\" Problem","text":"<ul> <li>Issue: Caching a complex object (e.g., a massive User Profile containing history, settings, and logs) when the application only requires a single field (e.g., <code>username</code>). This fills memory with \"dead weight\".</li> <li>Solution: Store only necessary data. Use Data Transfer Objects (DTOs) or flattened structures rather than caching full objects,.</li> </ul>"},{"location":"Caching/#b-optimization-the-active-user-strategy","title":"B. Optimization: The \"Active User\" Strategy","text":"<ul> <li>Issue: In a system with 1 million users, caching \"Recently Viewed Items\" (e.g., T-shirt_IDs) for everyone consumes massive memory, even for users who are inactive.</li> <li>Solution: Filter Active Users. Do not cache data for inactive users.</li> <li>Filtering Metrics:</li> <li>Frequency: Select top users by <code>NUM_OF_LOG_INS</code> (e.g., User_32124 has 297 logins).</li> <li>Recency: Select users with recent <code>LAST_LOG_IN</code> timestamps (e.g., User_32124 logged in at 21:43 today),.</li> </ul>"},{"location":"Caching/#c-locality-of-reference","title":"C. Locality of Reference","text":"<ul> <li>Temporal Locality: If a data item is accessed now, it will likely be accessed again soon (e.g., Loop counters),.</li> <li>Spatial Locality: If <code>Address X</code> is accessed, <code>Address X+1</code> will likely be accessed soon (e.g., Arrays).</li> <li>Working Set Size: This refers to the amount of memory a process needs at a given time. If the Cache is smaller than the Working Set, Thrashing occurs (constant eviction leading to 0% performance).</li> </ul>"},{"location":"Caching/#4-placement-of-caches","title":"4. Placement of Cache(s)","text":""},{"location":"Caching/#cache-hierarchy","title":"Cache Hierarchy","text":"<ul> <li>L1/L2/L3: CPU-level caches,.</li> <li>Client-Side: Browser cache (saves network calls),.</li> <li>CDN: Geographically distributed static content (reduces latency by being physically closer to the user),.</li> </ul>"},{"location":"Caching/#cache-mapping-internal-placement","title":"Cache Mapping (Internal Placement)","text":"<ul> <li>Direct Mapped: A block maps to 1 specific line. (Fastest, but high conflict).</li> <li>Fully Associative: A block maps to any line. (Slow search, zero conflict).</li> <li>Set Associative: A block maps to a specific set of lines. (Balance of speed and conflict).</li> </ul>"},{"location":"Caching/#5-distributed-caching-scaling","title":"5. Distributed Caching &amp; Scaling","text":"<p>When a system grows beyond a single server, where and how the cache is deployed becomes the critical determinant of performance.</p>"},{"location":"Caching/#a-deployment-strategies","title":"A. Deployment Strategies","text":""},{"location":"Caching/#1-in-memory-cache","title":"1. In-Memory Cache","text":"<ul> <li>Definition: The application server holds the cache locally within its own memory.</li> <li>Architecture: Each instance of the application maintains its own private cache.</li> <li>Pros: Zero network latency (fastest possible access).</li> <li>Cons:</li> <li>Data Duplication: The same data is stored across multiple app instances, wasting memory.</li> <li>Inconsistency: Instance A might hold different data than Instance B.</li> </ul>"},{"location":"Caching/#2-centralized-cache-distributed-cache","title":"2. Centralized Cache (Distributed Cache)","text":"<ul> <li>Definition: A separate, shared layer of cache servers (e.g., Redis Cluster) accessible by all application nodes.</li> <li>Pros: No Duplication. It acts as a single source of truth for cached data.</li> <li>Cons: Network latency is added for every data fetch.</li> </ul>"},{"location":"Caching/#b-the-scaling-problem-in-centralized-caching","title":"B. The \"Scaling Problem\" in Centralized Caching","text":"<p>To scale a centralized cache, Sharding (splitting data into buckets based on IDs) is used. However, standard sharding faces a major issue when the cluster changes.</p> <ul> <li>The Scenario: A cache server crashes, or a new server is added to handle load.</li> <li>The Problem: Standard hashing uses <code>Key % N_Servers</code>. Changing the number of servers (\\(N\\)) invalidates almost all existing mappings.</li> <li>Consequence:</li> <li>Massive cache misses occur.</li> <li>Cache Warmup/Readiness takes a long time (minutes to days).</li> <li>The Database is overwhelmed by traffic, leading to a Thundering Herd problem while the cache rebuilds.</li> </ul>"},{"location":"Caching/#c-the-solution-consistent-hashing","title":"C. The Solution: Consistent Hashing","text":"<ul> <li>Definition: A technique to map keys to cache nodes that minimizes key movement when the cluster resizes.</li> <li>Mechanism (The Ring):</li> <li>The Ring: Both Cache Servers and Data Keys are hashed onto the same circular range (0 to \\(2^{32}-1\\)).</li> <li>Placement: A key is stored on the next server found moving clockwise on the ring.</li> <li> <p>Selection Probability: The chance of a specific server being picked is \\(1/N\\).</p> </li> <li> <p>Behavior on Change:</p> </li> <li>Node Removed: Only the keys belonging to that specific node are moved to the next neighbor.</li> <li>Node Added: It only takes keys from its immediate successor.</li> <li> <p>Result: Result: Only \\(\\approx \\frac{K}{N}\\) keys need to be remapped (not the whole keyspace).</p> </li> <li> <p>Optimizing Skewness (Virtual Nodes):</p> </li> <li>Problem: Data might be unevenly distributed (one server holds too much data) due to bad random hashing.</li> <li> <p>Solution: Use Virtual Points (Virtual Nodes). Each physical server appears at multiple positions on the ring (e.g., \"Server A1\", \"Server A2\", \"Server A3\"). This spreads the load more evenly.</p> </li> <li> <p>Benefits:</p> </li> <li>Stability: Adding/Removing servers is easy and safe.</li> <li>Faster Recovery: Drastically reduces Cache Warmup time and increases Cache Readiness because the majority of the cache remains valid.</li> <li>Real-world usage: Amazon DynamoDB, Redis Clusters, Cassandra.</li> </ul> <p> </p>"},{"location":"Caching/#updated-summary-decision-matrix","title":"Updated Summary Decision Matrix","text":"I want to optimize for... Scenario Recommended Strategy Why? Freshness Social Media Feed LRU Eviction Users care about recency (freshness), not history,. Stability E-Commerce Catalog LFU Eviction Popular products (iPhone) stay popular for long periods,. Data Integrity Banking / Inventory Write-Through Policy Data Integrity is critical. Zero data loss is allowed,. Write Speed View Counters / Likes Write-Behind (Write-Back) Write Speed is critical. Losing a few \"likes\" is acceptable. Cache Efficiency Logs / Backups Write-Around Do not pollute the cache with data you won't read again. Memory Usage Limited Memory Filter Active Users Do not cache data for inactive users to save space. Zero Latency High Speed Req. In-Memory Cache (Local) Eliminates network calls. Scalability Large Scale Systems Centralized Cache with Consistent Hashing Ensures consistency and easy scaling. Resilience Adding/Removing Nodes Consistent Hashing Minimizes re-warmup time and prevents Thundering Herds."},{"location":"DNS/","title":"DNS","text":""},{"location":"DNS/#dns-server","title":"DNS server","text":"<p>Browser goes to ISP goes and check Distributed DNS servers like Amazon Route 53, etc which holds key value pair. For a given domain name it resolves to ip address and return it back and browser's uses that, for future it caches the value.</p>"},{"location":"Handling%20traffic%20surge/","title":"Handling Traffic Surges","text":""},{"location":"Handling%20traffic%20surge/#in-cases-of-some-kind-of-flash-sale-for-some-hoursdays-you-prepare-your-system-for-it","title":"In cases of some kind of flash sale for some hours/days, you prepare your system for it.","text":"<p>In context of e-commerce these are some common steps are:</p> <ul> <li>Estimate the expected scale: Talk to business folks and understand the expected network traffic on your platform during that time. This can be roughly estimated by number of followers on social media platform, Google Ads and other ads usual conversion like ads shown to 100k and there is 1% usual conversion. Currently registered users, active users, etc.</li> <li>Pre-emptive scaling: Few hours before that scale up your system and keep them ready. Monitor the vital metrics. Do some kind of load testing and monitor the metrics, if everything is behaving normally. Progressively load your system to correctly understand how much computing power you need.</li> <li>Some useful metrics to monitor: request latency, memory utilized, currently active users.</li> <li>Understand the access patterns: The pattern in which data is accessed in your system. If there's a data which will be shown to every user like collection of products which are on sale. keep them in cdn, their image links too in cdn. Keep your cache warmed up.</li> </ul>"},{"location":"Handling%20traffic%20surge/#in-case-of-sudden-jump-in-traffic-your-system-likely-to-crash-in-such-cases-how-do-you-handle-the-situation","title":"In case of sudden jump in traffic your system likely to crash, in such cases how do you handle the situation.","text":"<ul> <li>Begin identifying that are all the users who came were real users ?</li> <li>Some common patterns are, with same user ids there could be thousands of requests/minute, in this it could be from different ips too!(DDoS attack)</li> <li>Gate-keeping to avoid malicious attacks: Use Cloud providers or CDN offers solution to identify and stop them. Eg.: Cloudflare, AWS Web application firewall. On server side have a firewall looking at the requests.</li> <li>Graceful degradation incase of over-loaded requests: Serve high priority requests first and lower ones later or drop them with an error message.</li> <li>Discarding requests altogether if needed:</li> <li>In a request flow, at any node(application server/cache/db etc) if the upstream or downstream service is failing or restarting, it is best to discard requests to give time to recover for that service(s).</li> <li>Short-circuit: Downstream services are overloaded so current service starts failing the requests to the downstream services.</li> <li>Back-pressure: When the downstream service is overloaded and it starts failing requests from upstream services</li> <li>Usually an error is thrown which signifies that it is a temporary error rather than permanent, so maybe try after a minute or so.</li> </ul>"},{"location":"Handling%20traffic%20surge/#how-does-your-system-actually-handles-requests","title":"How does your system actually handles requests?","text":"<p>Without any configuration by default most of the servers follow a queue,i.e. FIFO pattern, and faces head-of-line blocking. To handle it one could:</p> <ul> <li>use Parallelism</li> <li>Parallel queues and servers.</li> <li> <p>Multiple VM in the servers.</p> </li> <li> <p>Concurrency</p> </li> <li>Same queue is used but multi-threaded approach is used to process requests out of order.( More time taking requests are being processed later or asynchronously)</li> <li>Uses protocol which supports having multiple request queues, like gRPC.</li> </ul>"},{"location":"Handling%20traffic%20surge/#to-future-proof-your-system-for-increase-surges-in-traffic","title":"To future proof your system for increase surges in traffic","text":"<p>Ratelimiting: Answer the ones which you could afford while drop the rest. As making some users happy is better than making everyone unhappy.</p> <p>Rate Limiting</p>"},{"location":"High%20Latency/","title":"High Latency","text":"<p>If the web app feels slow then check:</p> <ul> <li>CDN is working fine or not.</li> <li>Page speed is slow or what ?</li> <li>Any particular page/flow is slower ?</li> </ul> <p>Solutions:</p> <ul> <li>Move data closer to user</li> <li>Browser level and server level caching</li> </ul>"},{"location":"LoadBalancer/","title":"Load Balancing","text":""},{"location":"LoadBalancer/#when-we-move-from-a-single-instance-of-server-to-multiple-then-the-need-arises-to-manage-incoming-connection-requests","title":"When we move from a single instance of server to multiple then the need arises to manage incoming connection requests.","text":"<p>Multiple server instances are usually kept so that the load is distributed rather than a single point get overloaded.</p> <p>But these are some challenges:</p> <ul> <li>Redirecting requests when one of the instances crashes.</li> <li>Judiciously utilizing all the available resources.</li> </ul> <p>These are handled by load balancers.</p> <p>For example: Amazon ELB. You can use pre defined algo for load balancing or write your own too.</p> <p></p>"},{"location":"Microservices/","title":"Microservices","text":""},{"location":"Microservices/#common-problems-when-teams-scale-up","title":"Common problems when teams scale up.","text":"<ul> <li>New engineers need training and KT.</li> <li>Lot of non overlapping updates in stand ups.</li> <li>Feature development and deployment cycle is much longer than it should be.</li> <li>Lot of hand-holding for new engineers.</li> <li>Lack of isolation of concerns and ownership.</li> </ul>"},{"location":"Microservices/#common-solution-is-to-split-them-into-teams-and-code-into-micro-services-these-should-be-based-on-business-units","title":"Common solution is to split them into teams and code into micro services. These should be based on business units.","text":""},{"location":"Microservices/#benefits","title":"Benefits:","text":"<ul> <li>Shorter stand ups.</li> <li>Each team can have their own deployment cycle.</li> <li>Each team can scale and manage their services independently.</li> </ul>"},{"location":"Microservices/#drawbacks","title":"Drawbacks:","text":"<ul> <li>Lack of context sharing between teams</li> <li>Duplication of effort across teams for common functionalities.</li> <li>Communication b/w code becomes less efficient than earlier.</li> <li>Communication required b/w people of different teams becomes complex.</li> <li>Communication regrading code b/w people. Need good documentation, API contracts, tech specs, PRD. Publishing new changes in backward compatible format.</li> </ul>"},{"location":"Microservices/#things-to-avoid-when-breaking-monolith-to-microservice","title":"Things to avoid when breaking Monolith to Microservice","text":"<ul> <li>Different teams using different languages. This to avoid high learning curve for each team member. But if there's a very special requirement only then do it.</li> <li>Try to use the same technologies across the org. It's easier to manage.</li> <li>Keep an eye on DevOps, as it might go up. Once it's need increases only then have a dedicated team.</li> </ul>"},{"location":"Microservices/#anti-pattern-to-avoid","title":"Anti-pattern to avoid","text":"<ul> <li>When new requirement comes, rather than handling them to existing teams, you create a new team and their service. If the feature can be handled and doesn't attract further work then it's best existing team handles, otherwise it leads to Nano-services. Having more services than people!</li> </ul>"},{"location":"Microservices/#when-to-create-microservice","title":"When to create MicroService ?","text":"<ul> <li>Push the new feature to existing teams, if in future work related to that increases then only create a new team.</li> <li>Two Microservices that only interact with each can be merged into one micro service.</li> </ul>"},{"location":"Rate%20Limiting/","title":"Rate Limiting","text":""},{"location":"Rate%20Limiting/#rate-limiting-algorithms","title":"Rate Limiting Algorithms","text":"<ul> <li>Token Bucket: You have max requests/second set in cache, every time a new request comes in that second it decreases the counter(token) and when it is responded then the counter increased again. So we know that at max how many requests are handled at a time.</li> <li>Leaky Bucket: How much time each request is allowed is set, for ex. 1/10 of second, i.e. 100ms. So in buckets of 100ms only each request gets processed and until the turn comes they wait in queue. Constant rate of requests. (Not used much, very rare)</li> </ul>"},{"location":"Rate%20Limiting/#applying-rate-limiting","title":"Applying Rate Limiting","text":"<ul> <li>Local Rate Limiting: Multiple servers, having their own rate limiting set locally.Faster, but not efficient use of resources. Easy to implement</li> <li>Centralized Rate Limiting: One of the server controls the rate limiting. Efficient use of resources but network latency. Allows global control.</li> </ul>"},{"location":"Reliability/","title":"Reliability","text":""},{"location":"Reliability/#how-reliable-is-the-system","title":"How reliable is the system ?","text":"<ul> <li> <p>Notice any points of failure, which on failing can lead to collapse of entire system.</p> </li> <li> <p>Any external dependency is prone to failure.</p> </li> </ul> <p>To fix this:</p> <ul> <li>Have redundancies.</li> <li>Have Cloud providers for better reliability like for CDN, for storage etc.</li> <li>For external dependencies have backups of using some other external dependencies like multiple payment gateways.</li> <li>Have graceful shutdown. Show generic error message.</li> </ul>"},{"location":"System%20Design/","title":"System Design Foundations","text":""},{"location":"System%20Design/#for-any-design-decision-in-hld-use-these-parameters","title":"For any design decision in HLD use these parameters:","text":"<ul> <li>Fidelity (It is fulfilling the use case or purpose.)</li> <li>Simplicity (Is it simple to implement and manage)</li> <li>Cost effectiveness ( Is it cost effective for a given use case and scale)</li> </ul>"},{"location":"System%20Design/#for-a-given-amount-of-money-having-one-large-instance-will-give-lesser-computation-power-as-compared-to-many-smaller-instances","title":"For a given amount of money having one large instance will give lesser computation power as compared to many smaller instances.","text":""},{"location":"System%20Design/#arguments-for-many-small-instances-why-the-statement-holds-true","title":"Arguments for \"Many Small Instances\" (Why the statement holds true)","text":"<p>While raw pricing might be linear, \"effective\" power often favors horizontal scaling in specific scenarios:</p> <ul> <li>Granular Auto-Scaling:   With small instances, you can scale your fleet to match traffic demand very closely. If you have one massive instance, you are paying for 100% of capacity even if traffic drops to 10%. Smaller instances reduce idle waste.</li> <li>Hardware Limits &amp; Premium Pricing:   Once you push into \"extreme\" vertical scaling (e.g., instances with terabytes of RAM or 100+ cores), the price-performance ratio often degrades. You begin paying a premium for the specialized engineering required to keep that much density on a single board (NUMA architectures, etc.).</li> <li>Burst Allowances:   Some cloud families (like AWS <code>T</code> series) offer burst credits. A fleet of smaller instances might collectively accumulate a larger pool of burstable CPU credits than a single large instance, offering higher peak performance for short durations.</li> </ul>"},{"location":"System%20Design/#arguments-for-one-large-instance-why-the-statement-is-false","title":"Arguments for \"One Large Instance\" (Why the statement is false)","text":"<p>There are strong technical reasons why a single large instance often yields more actual computation power for the application than many small ones:</p> <ul> <li>The \"OS Tax\" (Overhead):   Every instance runs its own Operating System kernel, logging agents, monitoring sidecars, and networking stack.</li> <li>Scenario A (1 Large): You pay the OS memory/CPU overhead once.</li> <li>Scenario B (10 Small): You pay the OS overhead 10 times.</li> <li> <p>Result: The large instance leaves more resources available for your actual application code.</p> </li> <li> <p>Inter-Node Latency:   Communication between processes on the same machine (via shared memory or loopback) is nanoseconds/microseconds. Communication between distributed instances involves network hops (milliseconds). For tightly coupled workloads, distributed computing \"power\" is lost to network wait times.</p> </li> <li>Resource Fragmentation (Bin Packing):   If a single task requires 6GB of RAM and you have 4GB instances, you cannot run the task at all. A large instance absorbs \"lumpy\" resource requirements much better than small fragmented ones.</li> </ul>"},{"location":"System%20Design/#summary-table","title":"Summary Table","text":"Feature Many Small Instances (Scale Out) One Large Instance (Scale Up) Availability Higher. If one fails, only a fraction of capacity is lost. Lower. Single point of failure (SPOF). Cost Efficiency Higher for variable traffic (better auto-scaling). Higher for steady-state heavy processing (less OS overhead). Complexity High. Requires load balancers, service discovery, distributed logging. Low. Monolithic management. Ideal Workload Stateless web apps, microservices. Database primaries, in-memory caches, legacy monoliths."}]}